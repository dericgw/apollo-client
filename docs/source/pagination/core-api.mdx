---
title: Core pagination API
sidebar_title: Core API
---

## The `fetchMore` function

In Apollo, the easiest way to request additional pages of paginated data is a function called [`fetchMore`](../caching/advanced-topics/#incremental-loading-fetchmore), which is a method of the `ObservableQuery` class returned by `client.watchQuery`, and is also included in the object returned by the `useQuery` Hook.

In the `options` you pass to `fetchMore`, you specify the new `options.variables` and (optionally) an `options.query` to be used when fetching the additional data. If no `options.query` is specified, the original query passed to `client.watchQuery` or `useQuery` will be reused.

In Apollo Client 2, you would also provide an `options.updateQuery` function, which was responsible for merging fetched pages with existing data in the cache. In Apollo Client 3, custom `merge` functions allow this logic to be specified in a central location, rather than duplicated everywhere you call `fetchMore`.

## Combining pages with `merge` functions

> The example below uses offset-based pagination, but this article applies to all pagination strategies.

While it is possible to store individual pages of list data separately on the client, application code that consumes the data tends to be simpler if the client combines all the pages it has received so far into a single list, using field arguments to guide how new data is spliced together with existing data.

Suppose your schema defines the following types:

```graphql
extend type Query {
  feed(
    type: FeedType!,
    offset: Int,
    limit: Int,
  ): [FeedItem!]
}

enum FeedType {
  PERSONAL
  PUBLIC
}

type FeedItem {
  id: String!
  # ...
}
```

The query you use to fetch data for the `Query.feed` field might look something like this:
```ts
const FEED_QUERY = gql`
  query Feed($type: FeedType!, $offset: Int, $limit: Int) {
    feed(type: $type, offset: $offset, limit: $limit) {
      id
      # ... other FeedItem fields
    }
  }
`;
```

Without input from you, `InMemoryCache` has no way of knowing how to interpret these field arguments, so it must assume they are all important. For example, imagine writing two consecutive pages into a cache that does not know about your pagination system:

```ts
const cache = new InMemoryCache;

cache.writeQuery({
  query: FEED_QUERY,
  variables: {
    type: "PERSONAL",
    offset: 0,
    limit: 2,
  },
  data: {
    feed: [
      { __typename: "FeedItem", id: 1 },
      { __typename: "FeedItem", id: 2 },
    ]
  }
});

cache.writeQuery({
  query: FEED_QUERY,
  variables: {
    type: "PERSONAL",
    offset: 2, // Changed!
    limit: 2,
  },
  data: {
    feed: [
      { __typename: "FeedItem", id: 3 },
      { __typename: "FeedItem", id: 4 },
    ]
  }
});
```

The cache stores those two results separately by default, using keys derived from both the field name (`feed`) and the serialized arguments (`limit`, `offset`, and `type`):

```js
expect(cache.extract()).toEqual({
  // Internal, normalized InMemoryCache data:
  "FeedItem:1": { __typename: "FeedItem", id: 1 },
  "FeedItem:2": { __typename: "FeedItem", id: 2 },
  "FeedItem:3": { __typename: "FeedItem", id: 3 },
  "FeedItem:4": { __typename: "FeedItem", id: 4 },
  "ROOT_QUERY": {
    __typename: "Query",
    // Notice we end up with two separate pages, one per unique
    // combination of Query.feed field arguments.
    'feed({"limit":2,"offset":0,"type":"PERSONAL"})': [
      { __ref: "FeedItem:1" },
      { __ref: "FeedItem:2" },
    ],
    'feed({"limit":2,"offset":2,"type":"PERSONAL"})': [
      { __ref: "FeedItem:3" },
      { __ref: "FeedItem:4" },
    ],
  },
});
```

A simplistic way to accumulate a single list instead of storing separate lists is to concatenate the results together, using a `merge` function for the `Query.feed` field:

```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          // Stop using limit and offset to differentiate field keys,
          // but continue using type.
          keyArgs: ["type"],
          // If there is no existing value for this field, existing
          // will be undefined.
          merge(existing = [], incoming) {
            return [...existing, ...incoming];
          },
        }
      }
    }
  }
})
```

Now that the cache has a strategy for combining incoming pages with existing data, its internal representation will be a bit more compact:

```js
expect(cache.extract()).toEqual({
  "FeedItem:1": { __typename: "FeedItem", id: 1 },
  "FeedItem:2": { __typename: "FeedItem", id: 2 },
  "FeedItem:3": { __typename: "FeedItem", id: 3 },
  "FeedItem:4": { __typename: "FeedItem", id: 4 },
  "ROOT_QUERY": {
    __typename: "Query",
    // One consolidated list of Query.feed data (per type):
    'feed({"type":"PERSONAL"})': [
      { __ref: "FeedItem:1" },
      { __ref: "FeedItem:2" },
      { __ref: "FeedItem:3" },
      { __ref: "FeedItem:4" },
    ],
  },
});
```

However, this simple strategy makes some risky assumptions about the order of the written pages, because it ignores the `offset` and `limit` arguments. In order to handle those arguments correctly, your `merge` function should use `options.args` to decide where to put the incoming data:

```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          keyArgs: ["type"],
          merge(existing, incoming, { args: { offset = 0 }}) {
            // Slicing is necessary because the existing data is
            // immutable, and frozen in development.
            const merged = existing ? existing.slice(0) : [];
            for (let i = 0; i < incoming.length; ++i) {
              merged[offset + i] = incoming[i];
            }
            return merged;
          },
        },
      },
    },
  },
});
```

This logic handles sequential page writes the same way the concatenation strategy would, but it can also tolerate repeated, overlapping, or out-of-order writes, without duplicating any list items.


## Two kinds of `read` function

Now that you understand the basics of `merge` functions, you may be wondering about the other direction cache data can flow: what happens when you read from a paginated field?

Just as a `merge` function uses field arguments to guide how paginated data is reassembled from individual pages, maintaining a single combined list of internal cache data, a custom `read` function has the opportunity to translate that internal list back into individual pages, again using field arguments to select the page in question, and possibly even sort or filter the data, depending on what other arguments you choose to implement. This capability goes beyond returning the same pages that were originally received, since a `read` function for `offset`/`limit` pagination could read from any available `offset`, with any desired `limit`:

```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          read(existing, { args: { offset, limit }}) {
            // Important to return undefined when existing is undefined,
            // rather than defaulting to an empty array, because returning
            // undefined is how a read function signals that the field is
            // missing, causing the query to be fetched from the network.
            return existing && existing.slice(offset, offset + limit);
          },

          // The keyArgs and merge configurations are the same as above.
          keyArgs: ["type"],
          merge(existing, incoming, { args: { offset = 0 }}) {
            const merged = existing ? existing.slice(0) : [];
            for (let i = 0; i < incoming.length; ++i) {
              merged[offset + i] = incoming[i];
            }
            return merged;
          },
        },
      },
    },
  },
});
```

Depending on the assumptions you feel comfortable making, you may wish to make this code more defensive. For example, you might want to provide default values for the `offset` and `limit` arguments, in case someone tries to access the `Query.feed` field without using the appropriate arguments:

```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          read(existing, {
            args: {
              // Default to returning the whole array, if the offset and
              // limit arguments are not provided.
              offset = 0,
              limit = existing?.length,
            } = {},
          }) {
            return existing && existing.slice(offset, offset + limit);
          },
          // ... keyArgs, merge ...
        },
      },
    },
  },
});
```

This style of `read` function, which takes responsibility for re-paginating your data based on the field arguments, essentially inverts the behavior of the `merge` function, so your application can query different pages using different arguments. **However, this is not the only way to write a `read` function!**

Another reasonable approach to writing a `read` function for a paginated field like `Query.feed` is to _ignore_ the `offset` and `limit` arguments, and always return the entire list, so your application code can take responsibility for slicing the list into pages, depending on how you want to display the data in different parts of the application.

If you adopt this second approach, you may discover you don't even need a `read` function, because you no longer need to examine the `args` or slice the `existing` data before returning it. That's why the `offsetLimitPagination` helper we mentioned above is implemented without a `read` function.

> When you provide both a `merge` function and `read` function, `keyArgs: false` will be assumed by default (though it can be overridden to something else, like `keyArgs: ["type"]`). However, if you provide only a `merge` function (or only a `read` function), you should specify `keyArgs` explicitly, to override the default behavior of considering all arguments relevant.

Which approach is right for your application? While the answer may vary from field to field, the tradeoffs between these two approaches should become clearer once we introduce Apollo Client's primary pagination API: `fetchMore`.

## Which arguments belong in `keyArgs`?

Throughout this discussion, we've seen a number of possible `keyArgs` configurations, ranging from including all arguments by default, to completely disabling argument-based field identification using `keyArgs: false`.

To understand which arguments belong in `keyArgs` (if any), it's helpful to consider those two extremes first (including all arguments in the field key, or none of them), because those are the most common cases. Building on that understanding, we can then discuss the consequences of moving an individual argument into or out of `keyArgs`.

If you include all arguments in the field key, as `InMemoryCache` does by default, then every different combination of argument values will correspond to a different storage location for internal field data. In other words, if you change any argument values, the field key will be different, so the field value will be stored in a different location. In your `read` and `merge` functions, this internal field data is provided by the `existing` parameter, which will be undefined when a particular combination of arguments has never been seen before. With this approach, the cache can reuse field values only if the arguments exactly match, which significantly reduces the hit rate of the cache, but also keeps the cache from inappropriately reusing field values when differences in arguments actually matter.

On the other hand, if you configure your field with `keyArgs: false`, the field key will always be just the field name, without any extra characters appended to it. Because your `read` and `merge` functions have access to the field arguments via `options.args`, you could keep your internal data separated according to those arguments, simulating the behavior of `keyArgs` without actually using `keyArgs`. Your `read` function then gets to decide whether an existing field value can be reused, based on the runtime argument values and whatever internal data have previously been stored for the field.

For example, we could have used `keyArgs: false` instead of `keyArgs: ["type"]` for our `Query.feed` field policy:

```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          keyArgs: false,

          read(existing = {}, { args: { type, offset, limit }}) {
            return existing[type] &&
              existing[type].slice(offset, offset + limit);
          },

          merge(existing = {}, incoming, { args: { type, offset = 0 }}) {
            const merged = existing[type] ? existing[type].slice(0) : [];
            for (let i = 0; i < incoming.length; ++i) {
              merged[offset + i] = incoming[i];
            }
            existing[type] = merged;
            return existing;
          },
        },
      },
    },
  },
});
```

Instead of a single array, `existing` is now a map from `type`s to feeds, allowing a single field value to store multiple feed arrays, separated by `type`. However, this manual separation is logically equivalent to what would happen if you moved the `type` argument into `keyArgs` (using `keyArgs: ["type"]`, as above), so the extra effort is probably unnecessary.

Assuming feeds with different `type` values are independent, and our `read` function does not need simultaneous access to multiple feeds of different types, we can safely shift the responsibility for handling the `type` argument from the `read` and `merge` functions back to `keyArgs`, and simplify `read` and `merge` to handle only one feed at a time.

In short, if the logic for storing and retrieving field data is the same for different values of a given argument (like `type`), and those field values are logically independent from one another, then you probably should move that argument into `keyArgs`, to save yourself from having to deal with it in your `read` and `merge` functions.

By contrast, arguments that limit, filter, sort, or otherwise reprocess existing field data usually do not belong in `keyArgs`, because putting them in `keyArgs` fragments the internal data, so you cannot use a different argument value to retrieve a different view of the same data, without making a network request.

As a general rule, `read` and `merge` functions can do almost anything with your field data, but there might be a less powerful tool (like `keyArgs`) that allows you to simplify (or avoid writing) custom `read` or `merge` functions. Whenever you have a choice between two capable tools, prefer the one that minimizes the total complexity of your code, while still providing the behavior you desire.

## The `@connection` directive

The `@connection` directive is a Relay-inspired convention that Apollo Client supports, though we now recommend `keyArgs` instead, because you can achieve the same effect with a single `keyArgs` configuration, whereas the `@connection` directive needs to be repeated in every query you send to your server.

In other words, whereas Relay encourages the following `@connection(...)` directive for `Query.feed` queries:
```js
const FEED_QUERY = gql`
  query Feed($type: FeedType!, $offset: Int, $limit: Int) {
    feed(type: $type, offset: $offset, limit: $limit) @connection(
      key: "feed",
      filter: ["type"]
    ) {
      edges {
        node { ... }
      }
      pageInfo {
        endCursor
        hasNextPage
      }
    }
  }
`;
```
in Apollo Client, you would use the following query:
```js
const FEED_QUERY = gql`
  query Feed($type: FeedType!, $offset: Int, $limit: Int) {
    feed(type: $type, offset: $offset, limit: $limit) {
      edges {
        node { ... }
      }
      pageInfo {
        endCursor
        hasNextPage
      }
    }
  }
`;
```
and instead configure `keyArgs` in your `Query.feed` field policy:
```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: {
          keyArgs: ["type"],
        },
      },
    },
  },
})
```
Although `keyArgs` (and `@connection`) are useful for more than just paginated fields, it's worth noting that `relayStylePagination` configures `keyArgs: false` by default. You can reconfigure this `keyArgs` behavior by passing an alternate value to `relayStylePagination`:
```js
const cache = new InMemoryCache({
  typePolicies: {
    Query: {
      fields: {
        feed: relayStylePagination(["type"]),
      },
    },
  },
})
```
In the unlikely event that a `keyArgs` array is insufficient to capture the identity of a field, remember that you can pass a function for `keyArgs`, which allows you to serialize the `args` object however you want.
